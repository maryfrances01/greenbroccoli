{"organization":"NVIDIA","name":"NVIDIA TensorRT Inference Server (Model Serving)","homepage_url":"https://developer.nvidia.com/tensorrt","open_source":true,"repo_url":"https://github.com/NVIDIA/tensorrt-inference-server","logo":"nvidia.svg","twitter":"https://twitter.com/nvidiaai","crunchbase":"https://www.crunchbase.com/organization/no-data","components":null,"allow_duplicate_repo":true,"github_data":{"languages":[{"name":"C++","value":2063478,"color":"#f34b7d"},{"name":"Python","value":1400147,"color":"#3572A5"},{"name":"Shell","value":646156,"color":"#89e051"},{"name":"CMake","value":110460},{"name":"Roff","value":39687,"color":"#ecdebe"},{"name":"Smarty","value":10473},{"name":"C","value":3799,"color":"#555555"},{"name":"Cuda","value":3334,"color":"#3A4E3A"},{"name":"Dockerfile","value":1597,"color":"#384d54"}],"contributions":"34;25;32;28;22;17;26;21;18;9;17;15;14;7;17;25;19;16;13;17;15;14;16;25;16;14;18;13;11;8;11;2;0;1;11;11;13;12;13;9;15;20;7;9;11;12;15;13;10;9;20;13","firstWeek":"2020-05-17Z","stars":2209,"license":"Other","description":"The Triton Inference Server provides an optimized cloud and edge inferencing solution. ","latest_commit_date":"2021-05-14T03:15:30Z","latest_commit_link":"/triton-inference-server/server/commit/58e471473aaa6069da028beec4da3d288dc7144f","release_date":"2021-04-27T17:19:30Z","release_link":"https://github.com/NVIDIA/tensorrt-inference-server/releases","contributors_count":52,"contributors_link":"https://github.com/NVIDIA/tensorrt-inference-server/graphs/contributors"},"github_start_commit_data":{"start_commit_link":"/triton-inference-server/server/commit/e4c4d7b06743908d046867c9ece5b1b75c83669f","start_date":"2018-11-05T21:51:20Z"},"image_data":{"fileName":"nvidia-tensor-rt-inference-server-model-serving.svg","hash":"oexU5oK5XAIo8KWriaM6PznvG6ByYNQhXO2+kRZn/vw="},"firstCommitDate":"2018-11-05T21:51:20Z","firstCommitLink":"https://github.com/triton-inference-server/server/commit/e4c4d7b06743908d046867c9ece5b1b75c83669f","latestCommitDate":{"text":"about a month","value":"960","original":"2021-05-14T03:15:30Z"},"latestCommitLink":"https://github.com/triton-inference-server/server/commit/58e471473aaa6069da028beec4da3d288dc7144f","releaseDate":{"text":"2 months ago","value":"950","original":"2021-04-27T17:19:30Z"},"releaseLink":"https://github.com/NVIDIA/tensorrt-inference-server/releases","commitsThisYear":779,"contributorsCount":52,"contributorsLink":"https://github.com/NVIDIA/tensorrt-inference-server/graphs/contributors","language":"C++","stars":2209,"license":"Other","headquarters":"N/A","latestTweetDate":{"text":"about a month","value":"960","original":"2021-05-13T21:00:05.000Z"},"description":"The Triton Inference Server provides an optimized cloud and edge inferencing solution. ","crunchbaseData":{"acquisitions":[],"funding":"","description":"","homepage":"","city":"","region":"","country":"","twitter":"","linkedin":"","ticker":"","kind":"","numEmployeesMin":"","numEmployeesMax":""},"path":"AI and Machine Learning Lifecycle / Model Serving","landscape":"AI and Machine Learning Lifecycle / Model Serving","category":"AI and Machine Learning Lifecycle","amount":"N/A","oss":true,"href":"logos/nvidia-tensor-rt-inference-server-model-serving.svg","bestPracticeBadgeId":false,"bestPracticePercentage":null,"industries":[],"id":"nvidia-tensor-rt-inference-server-model-serving","flatName":"NVIDIA TensorRT Inference Server (Model Serving)","member":false,"relation":false,"isSubsidiaryProject":false}